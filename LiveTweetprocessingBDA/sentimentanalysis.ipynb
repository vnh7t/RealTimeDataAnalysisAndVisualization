{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YiDDqLxwyCWu",
    "outputId": "379a4446-b116-47f6-fc7f-6af8e734c90c"
   },
   "outputs": [],
   "source": [
    "pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BAGai2W9w1dE"
   },
   "outputs": [],
   "source": [
    "# importing required libraries\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.streaming import StreamingContext\n",
    "import pyspark.sql.types as tp\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, IndexToString\n",
    "from pyspark.ml.feature import StopWordsRemover, Word2Vec, RegexTokenizer, CountVectorizer, IDF\n",
    "from pyspark.ml.classification import LogisticRegression, NaiveBayes\n",
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JHbZFSBAxztI"
   },
   "outputs": [],
   "source": [
    "# initializing spark session\n",
    "sc = SparkContext(appName=\"PySparkShell\")\n",
    "spark = SparkSession(sc)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ooMm5zOpypyC"
   },
   "outputs": [],
   "source": [
    "# define the schema\n",
    "my_schema = tp.StructType([\n",
    "  tp.StructField(name= 'label',       dataType= tp.IntegerType(),  nullable= True),\n",
    "  tp.StructField(name= 'id',          dataType= tp.IntegerType(),  nullable= True),\n",
    "  tp.StructField(name= 'date',          dataType= tp.StringType(),  nullable= True),\n",
    "  tp.StructField(name= 'NO_QUERY',       dataType= tp.StringType(),  nullable= True),\n",
    "  tp.StructField(name= 'username',       dataType= tp.StringType(),  nullable= True),\n",
    "  tp.StructField(name= 'tweet',       dataType= tp.StringType(),   nullable= True)\n",
    "])\n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2psU0JNRyr9O"
   },
   "outputs": [],
   "source": [
    " \n",
    "# read the dataset  \n",
    "my_data = spark.read.csv('/Users/venkatavarunnelakuditi/Downloads/training1600000.csv',\n",
    "                         schema=my_schema,\n",
    "                         header=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O_zs8JNY6B68"
   },
   "outputs": [],
   "source": [
    "my_data=my_data.drop(\"id\",\"date\",\"NO_QUERY\",\"username\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zT6rlwqbywuB",
    "outputId": "55398453-0595-42cc-9b5d-1e364f0b3c37"
   },
   "outputs": [],
   "source": [
    "# view the data\n",
    "my_data.show(5)\n",
    "\n",
    "# print the schema of the file\n",
    "my_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SFBxR9ys7XVC"
   },
   "outputs": [],
   "source": [
    "# define stage 1: tokenize the tweet text    \n",
    "stage_1 = RegexTokenizer(inputCol= 'tweet' , outputCol= 'tokens', pattern= '\\\\W')\n",
    "# define stage 2: remove the stop words\n",
    "stage_2 = StopWordsRemover(inputCol= 'tokens', outputCol= 'filtered_words')\n",
    "# define stage 3: create a word vector of the size 100\n",
    "# bag of words count\n",
    "#stage_3 = CountVectorizer(inputCol=\"filtered_words\", outputCol=\"cf\", vocabSize=20000, minDF=5)\n",
    "#hashtf = HashingTF(numFeatures=2 ** 16, inputCol=\"wordsWithStopwordsfree\", outputCol=\"tf\")\n",
    "#stage_4 = IDF(inputCol=\"cf\", outputCol=\"vector\", minDocFreq=5)\n",
    "stage_3 = Word2Vec(inputCol= 'filtered_words', outputCol= 'vector', vectorSize= 100)\n",
    "# define stage 4: Logistic Regression Model\n",
    "model = LogisticRegression(featuresCol= 'vector', labelCol= 'label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B9pGYIy88WOd"
   },
   "outputs": [],
   "source": [
    "\n",
    "# setup the pipeline\n",
    "pipeline = Pipeline(stages= [stage_1, stage_2, stage_3, model])\n",
    "\n",
    "# fit the pipeline model with the training data\n",
    "pipelineFit = pipeline.fit(my_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G8bFbIprGPNQ"
   },
   "outputs": [],
   "source": [
    "pipelineFit.save(\"preprocessingAndLR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the dataset  \n",
    "bbc_data = spark.read.csv('/Users/venkatavarunnelakuditi/Downloads/BBCNewsTrain.csv',\n",
    "                         header=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbc_data.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbc_data=bbc_data.drop('ArticleId')\n",
    "bbc_data=bbc_data.withColumnRenamed(\"Text\",\"tweet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n",
    "bbc_data = bbc_data.withColumn(\"Category\", when(bbc_data.Category == \"tech\",0) \\\n",
    "      .when(bbc_data.Category == \"business\",1).when(bbc_data.Category == \"politics\",2).when(bbc_data.Category == \"sport\",3).when(bbc_data.Category == \"entertainment\",4).otherwise(0))\n",
    "\n",
    "#indexers = [StringIndexer(inputCol=\"Category\", outputCol=\"Category_index\")]\n",
    "\n",
    "\n",
    "#pipeline = Pipeline(stages=indexers)\n",
    "#bbc_data = pipeline.fit(bbc_data).transform(bbc_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbc_data.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# define stage 1: tokenize the tweet text    \n",
    "stage_1 = RegexTokenizer(inputCol= 'tweet' , outputCol= 'tokens', pattern= '\\\\W')\n",
    "# define stage 2: remove the stop words\n",
    "stage_2 = StopWordsRemover(inputCol= 'tokens', outputCol= 'filtered_words')\n",
    "# define stage 3: create a word vector of the size 100\n",
    "#stage_3 = Word2Vec(inputCol= 'filtered_words', outputCol= 'vector')\n",
    "\n",
    "# bag of words count\n",
    "stage_3 = CountVectorizer(inputCol=\"filtered_words\", outputCol=\"cf\", vocabSize=20000, minDF=5)\n",
    "#hashtf = HashingTF(numFeatures=2 ** 16, inputCol=\"wordsWithStopwordsfree\", outputCol=\"tf\")\n",
    "stage_4 = IDF(inputCol=\"cf\", outputCol=\"vector\", minDocFreq=5)\n",
    "# define stage 4: Logistic Regression Model\n",
    "model = NaiveBayes(featuresCol= 'vector', labelCol= 'Category',smoothing=1.0, modelType='multinomial')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup the pipeline\n",
    "pipelineBBC = Pipeline(stages= [stage_1, stage_2, stage_3,stage_4,model])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF,testDF=bbc_data.randomSplit([0.75, 0.25], seed=2000)\n",
    "# fit the pipeline model with the training data\n",
    "BBCpipelineFit = pipelineBBC.fit(trainDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = BBCpipelineFit.transform(testDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "\n",
    "test_df=test_df.withColumnRenamed(\"Category\",\"label\")\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "evaluator.evaluate(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BBCpipelineFit.save(\"preprocessingAndCategory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbc_data.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "sentimentanalysis.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
